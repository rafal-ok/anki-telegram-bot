# Rename this file to .env and fill in your values
TELEGRAM_BOT_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
# Generation backend:
# - codex-ui-queue: queue sources for Codex UI / Codex workflows (no OpenAI API calls)
# - openai-api: generate cards immediately from text/photos via OpenAI API
# Legacy aliases still accepted: codex, openai
CARD_GENERATION_BACKEND=codex-ui-queue

# Required only when CARD_GENERATION_BACKEND=openai-api
OPENAI_API_KEY=sk-proj-...
OPENAI_VISION_MODEL=gpt-4o-mini

ANKI_LANG=en
DEFAULT_DECK_NAME=Telegram Imports
DB_PATH=anki_bot.db
EXPORT_DIR=exports
SOURCE_DIR=sources
AUTO_PROPOSE_FROM_TEXT=1
MAX_SOURCE_TEXT_CHARS=12000
MAX_DOCUMENT_BYTES=8000000
BACKUP_DIR=backups

# Proposal generation backend for /propose:
# - auto (recommended): try codex-cli, then ollama-local-mac, then heuristic fallback
# - codex-cli: Codex CLI only (no direct API calls from this bot)
# - ollama-local-mac: local LLM via Ollama only
# - openai-api: OpenAI Responses API only
# - heuristic: fallback non-LLM rules
# Legacy aliases still accepted: codex, ollama, openai
PROPOSAL_GENERATION_BACKEND=auto
PROPOSAL_MAX_NOTES=3
PROPOSAL_CODEX_CMD=codex
PROPOSAL_CODEX_MODEL=
PROPOSAL_CODEX_TIMEOUT_SEC=90
# Optional path for legacy Codex CLI fallback behavior
PROPOSAL_CODEX_CLI_JS=/opt/homebrew/lib/node_modules/@openai/codex/dist/cli.js
# Security controls:
# - strict workspace only: force Codex to run in this project with no extra sandbox permissions
# - legacy fallback: keep disabled unless you intentionally use a very old Codex CLI
PROPOSAL_CODEX_STRICT_WORKSPACE_ONLY=1
PROPOSAL_CODEX_ALLOW_LEGACY_FALLBACK=0
PROPOSAL_OLLAMA_BASE_URL=http://127.0.0.1:11434
PROPOSAL_OLLAMA_MODEL=llama3.2:3b
PROPOSAL_OLLAMA_TIMEOUT_SEC=120

# Optional Mochi defaults (used when user-level values are not set via Telegram commands)
MOCHI_API_KEY=
MOCHI_DECK_ID=
MOCHI_SYNC_FETCH_LIMIT=100

# Optional: HTTP/SOCKS proxies if needed by python-telegram-bot (format as requests lib supports)
# HTTPS_PROXY=
# HTTP_PROXY=
